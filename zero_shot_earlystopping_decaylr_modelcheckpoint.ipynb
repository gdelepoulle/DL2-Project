{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"zero_shot_earlystopping_decaylr_modelcheckpoint.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AWid5RiiYykE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwsC0bk__QxA"},"source":["path_prefix = \"/content/drive/My Drive/zero_shot/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"peX6wWd2Mmgw"},"source":["import numpy as np\n","\n","import os\n","from collections import Counter\n","\n","from sklearn.preprocessing import LabelEncoder, normalize\n","from sklearn.neighbors import KDTree\n","import tensorflow as tf\n","from keras.models import Sequential, Model, model_from_json\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import BatchNormalization\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.applications.vgg16 import VGG16\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","np.random.seed(1234)\n","WORD2VECPATH    = path_prefix + \"data/class_vectors.npy\"\n","MODELPATH       = path_prefix + \"DatasetA_train_20180813/model/\"\n","MODELNAME = \"modelV3\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1y6mv75Gjnj"},"source":["#set LR decay\n","def scheduler0(epoch, lr):\n","    return lr\n","\n","def scheduler1(epoch, lr):\n","  if epoch < 20:\n","    return lr\n","  else:\n","    return lr * tf.math.exp(-0.1)\n","\n","def scheduler2(epoch, lr):\n","  if epoch < 20:\n","    return lr\n","  else:\n","    return lr * tf.math.exp(-0.05)\n","\n","def scheduler3(epoch, lr):\n","  if epoch < 20:\n","    return lr\n","  else:\n","    return lr * tf.math.exp(-0.01)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UObAlQh_MtGn"},"source":["def save_keras_model(model, model_path):\n","    \"\"\"save Keras model and its weights\"\"\"\n","    if not os.path.exists(model_path):\n","        os.makedirs(model_path)\n","\n","    model_json = model.to_json()\n","    with open(model_path + MODELNAME +  \".json\", \"w\") as json_file:\n","        json_file.write(model_json)\n","\n","    # serialize weights to HDF5\n","    model.save_weights(model_path + MODELNAME + \".h5\")\n","    print(\"-> zsl model is saved.\")\n","    return\n","\n","def load_keras_model(model_path):\n","    with open(model_path + MODELNAME + \".json\", 'r') as json_file:\n","        loaded_model_json = json_file.read()\n","\n","    loaded_model = model_from_json(loaded_model_json)\n","    # load weights into new model\n","    loaded_model.load_weights(model_path + MODELNAME + \".h5\")\n","    return loaded_model\n","\n","def build_model():\n","    model  = Sequential()\n","    base_model = VGG16(include_top=False, input_shape=(224,224,3))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","    model.add(base_model)\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(NUM_ATTR, activation='relu'))\n","    model.add(Dense(NUM_CLASS, activation='softmax', trainable=False))#, kernel_initializer=custom_kernel_init))\n","    return model\n","\n","\n","def train_model(model, train_ds, val_ds, lr_value, decay):\n","\n","    #Add decay\n","    if decay == 'None':\n","        reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler0)\n","    elif decay == 'exp-0.1':\n","        reduce_lr=tf.keras.callbacks.LearningRateScheduler(scheduler1)\n","    elif decay == 'exp-0.05':\n","        reduce_lr=tf.keras.callbacks.LearningRateScheduler(scheduler2)\n","    elif decay == 'exp-0.01':\n","        reduce_lr=tf.keras.callbacks.LearningRateScheduler(scheduler3)\n","\n","\n","    adam = Adam(lr= lr_value)\n","    model.compile(loss      = 'categorical_crossentropy',\n","                  optimizer = adam,\n","                  metrics   = ['categorical_accuracy', 'top_k_categorical_accuracy'])\n","    \n","    #Add early stopping\n","    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n","    fname_modelFile = 'modelV3.h5'\n","    mod_path = path_prefix + \"DatasetA_train_20180813/model/\" + fname_modelFile\n","    mc = ModelCheckpoint(mod_path, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\n","\n","    history = model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=EPOCH,\n","        callbacks=[early_stop, mc, reduce_lr]\n","    )\n","\n","    print(\"model training is completed at epoch \" + str(early_stop.stopped_epoch))\n","    return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RLlbywhYDg1"},"source":["global train_classes\n","with open(path_prefix + 'DatasetA_train_20180813/label_list.txt', 'r') as infile:\n","    name_classes = [str.strip(line).split('\\t') for line in infile]\n","  #Lets take 30 as Zsl classes\n","global zsl_classes\n","indexes = np.arange(0, len(name_classes))\n","zsl_indexes = np.random.choice(indexes, size=30, replace=False)\n","zsl_classes = np.array(name_classes)[zsl_indexes]\n","train_classes= []\n","for i,obj in enumerate(name_classes):\n","    if not obj in zsl_classes:\n","        train_classes.append(obj)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBmMuB8SCltV"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wl_DrzwRDT16"},"source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2, subset=\"training\", image_size=(224,224), seed=1234, batch_size=BATCH_SIZE, label_mode='categorical')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMJTmc21Eqh4"},"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2, image_size=(224,224), subset=\"validation\", seed=1234, batch_size=BATCH_SIZE, label_mode='categorical')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc5hKpN9DT55"},"source":["val_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cola3iEH9aBy"},"source":["# ---------------------------------------------------------------------------------------------------------------- #\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# SET HYPERPARAMETERS\n","\n","global NUM_CLASS, NUM_ATTR, EPOCH, BATCH_SIZE, LR_VAL_list, learnR_d, scheduler0, scheduler1, scheduler2, scheduler3\n","NUM_CLASS = 200\n","NUM_ATTR = 300\n","BATCH_SIZE = 128\n","EPOCH = 500\n","#LR_VAL_list = [1e-3, 1e-4, 5e-5]\n","LR_VAL = 5e-4\n","decay = 'exp-0.01'\n","\n","\n","\n","#DATA\n","data_dir = path_prefix + 'data/ordered_data/training'\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2, subset=\"training\", image_size=(224,224), seed=1234, batch_size=BATCH_SIZE, label_mode='categorical')\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2, image_size=(224,224), subset=\"validation\", seed=1234, batch_size=BATCH_SIZE, label_mode='categorical')\n","\n","\n","### Hyperparameters tuning ###\n","# for LR_VAL in LR_VAL_list:\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# TRAINING PHASE\n","\n","model = build_model()\n","model.summary()\n","train_model(model, train_ds, val_ds, LR_VAL, decay )\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# CREATE AND SAVE ZSL MODEL\n","\n","inp         = model.input\n","out         = model.layers[-2].output\n","zsl_model   = Model(inp, out)\n","print(zsl_model.summary())\n","#MODELNAME = \"V3_lr\" + str(LR_VAL) + \"_\"+str(lr_decay)\n","save_keras_model(zsl_model, model_path=MODELPATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8KFZR2rXov2"},"source":["zsl_dir = path_prefix + 'data/ordered_data/zeroshot'\n","\n","zsl_ds = tf.keras.preprocessing.image_dataset_from_directory(zsl_dir, seed=1234, image_size=(224,224))\n","\n","print(zsl_ds.class_names)\n","zsl_labels = zsl_ds.class_names\n","zsl_class_dict = {}\n","\n","for label_class in zsl_classes:\n","    zsl_class_dict[label_class[0]] = label_class[1]\n","\n","print(zsl_class_dict)\n","\n","zsl_ds = zsl_ds.take(1)\n","\n","zsl_model = load_keras_model(model_path=MODELPATH)\n","\n","with open(path_prefix + 'DatasetA_train_20180813/class_wordembeddings.txt', 'r') as infile:\n","    class_wordembeddings = [str.strip(line).split(' ') for line in infile]\n","\n","with open(path_prefix + 'DatasetA_train_20180813/label_list.txt', 'r') as infile:\n","    name_classes = [str.strip(line).split('\\t') for line in infile]\n","\n","sorted_embedings = [embed for x in name_classes for embed in class_wordembeddings if embed[0] == x[1]]\n","\n","vectors = np.array(sorted_embedings)[:,1:]\n","vectors = np.asarray(vectors, dtype=np.float)\n","\n","classnames = list(np.array(sorted_embedings)[:,0])\n","\n","\n","tree        = KDTree(vectors)\n","pred_zsl    = zsl_model.predict(zsl_ds)\n","\n","\n","print(\"Able to predict :)\")\n","\n","top5, top3, top1 = 0, 0, 0\n","\n","print(name_classes)\n","print(zsl_classes)\n","\n","for images in zsl_ds:\n","    print(images[0].numpy().shape)\n","    print(images[1])\n","    for i in range(32):\n","        prediction = pred_zsl[i]\n","        image = images[0].numpy()[i]\n","        pred = np.expand_dims(prediction, axis=0)\n","        dist_5, index_5 = tree.query(pred, k=5)\n","        pred_labels = [classnames[index] for index in index_5[0]]\n","        actual_label = zsl_class_dict[zsl_labels[images[1][i]]]\n","        print(zsl_classes[images[1][i]])\n","        # image = 1. - image / 127.5\n","        plt.figure()\n","        plt.imshow(image.astype(\"uint8\"))\n","        plt.title(\"predicted labels for \" + str(i) + \": \" + str(pred_labels) + \" actual label: \" + actual_label)\n","\n"],"execution_count":null,"outputs":[]}]}